{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, encode_dim, batch_size, use_gpu, pretrained_weight=None):\n",
    "        super(BatchTreeEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.W_c = nn.Linear(embedding_dim, encode_dim)\n",
    "        self.activation = F.relu\n",
    "        self.stop = -1\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "        self.node_list = []\n",
    "        self.th = torch.cuda if use_gpu else torch\n",
    "        self.batch_node = None\n",
    "        self.max_index = vocab_size\n",
    "        # pretrained  embedding\n",
    "        if pretrained_weight is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "            # self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def create_tensor(self, tensor):\n",
    "        if self.use_gpu:\n",
    "            return tensor.cuda()\n",
    "        return tensor\n",
    "\n",
    "    def traverse_mul(self, node, batch_index):\n",
    "        size = len(node)\n",
    "        if not size:\n",
    "            return None\n",
    "        batch_current = self.create_tensor(Variable(torch.zeros(size, self.embedding_dim)))\n",
    "\n",
    "        index, children_index = [], []\n",
    "        current_node, children = [], []\n",
    "        for i in range(size):\n",
    "            # if node[i][0] is not -1:\n",
    "                index.append(i)\n",
    "                current_node.append(node[i][0])\n",
    "                temp = node[i][1:]\n",
    "                c_num = len(temp)\n",
    "                for j in range(c_num):\n",
    "                    if temp[j][0] != -1:\n",
    "                        if len(children_index) <= j:\n",
    "                            children_index.append([i])\n",
    "                            children.append([temp[j]])\n",
    "                        else:\n",
    "                            children_index[j].append(i)\n",
    "                            children[j].append(temp[j])\n",
    "            # else:\n",
    "            #     batch_index[i] = -1\n",
    "\n",
    "        batch_current = self.W_c(batch_current.index_copy(0, Variable(self.th.LongTensor(index)),\n",
    "                                                          self.embedding(Variable(self.th.LongTensor(current_node)))))\n",
    "\n",
    "        for c in range(len(children)):\n",
    "            zeros = self.create_tensor(Variable(torch.zeros(size, self.encode_dim)))\n",
    "            batch_children_index = [batch_index[i] for i in children_index[c]]\n",
    "            tree = self.traverse_mul(children[c], batch_children_index)\n",
    "            if tree is not None:\n",
    "                batch_current += zeros.index_copy(0, Variable(self.th.LongTensor(children_index[c])), tree)\n",
    "        # batch_index = [i for i in batch_index if i is not -1]\n",
    "        b_in = Variable(self.th.LongTensor(batch_index))\n",
    "        self.node_list.append(self.batch_node.index_copy(0, b_in, batch_current))\n",
    "        return batch_current\n",
    "\n",
    "    def forward(self, x, bs):\n",
    "        self.batch_size = bs\n",
    "        self.batch_node = self.create_tensor(Variable(torch.zeros(self.batch_size, self.encode_dim)))\n",
    "        self.node_list = []\n",
    "        self.traverse_mul(x, list(range(self.batch_size)))\n",
    "        self.node_list = torch.stack(self.node_list)\n",
    "        return torch.max(self.node_list, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProgramClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, encode_dim, label_size, batch_size,\n",
    "                 use_gpu=True, pretrained_weight=None):\n",
    "        super(BatchProgramClassifier, self).__init__()\n",
    "        self.additionl = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.additionr = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.stop = [vocab_size-1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = 1\n",
    "        self.gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.label_size = label_size\n",
    "        self.encoder = BatchTreeEncoder(self.vocab_size, self.embedding_dim, self.encode_dim,\n",
    "                                        self.batch_size, self.gpu, pretrained_weight)\n",
    "        self.root2label = nn.Linear(self.encode_dim, self.label_size)\n",
    "        # gru\n",
    "        self.bigru = nn.GRU(self.encode_dim, self.hidden_dim, num_layers=self.num_layers, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        # linear\n",
    "        self.hidden2label = nn.Linear(self.hidden_dim * 2, self.label_size)\n",
    "        # hidden\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.gpu is True:\n",
    "            if isinstance(self.bigru, nn.LSTM):\n",
    "                h0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                c0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                return h0, c0\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def get_zeros(self, num):\n",
    "        zeros = Variable(torch.zeros(num, self.encode_dim))\n",
    "        if self.gpu:\n",
    "            return zeros.cuda()\n",
    "        return zeros\n",
    "\n",
    "    def encode(self, x):\n",
    "        lens = [len(item) for item in x]\n",
    "        max_len = max(lens)\n",
    "\n",
    "        encodes = []\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(lens[i]):\n",
    "                encodes.append(x[i][j])\n",
    "\n",
    "        encodes = self.encoder(encodes, sum(lens))\n",
    "        seq, start, end = [], 0, 0\n",
    "        for i in range(self.batch_size):\n",
    "            end += lens[i]\n",
    "            if max_len-lens[i]:\n",
    "                seq.append(self.get_zeros(max_len-lens[i]))\n",
    "            seq.append(encodes[start:end])\n",
    "            start = end\n",
    "        encodes = torch.cat(seq)\n",
    "        encodes = encodes.view(self.batch_size, max_len, -1)\n",
    "        # return encodes\n",
    "\n",
    "        gru_out, hidden = self.bigru(encodes, self.hidden)\n",
    "        gru_out = torch.transpose(gru_out, 1, 2)\n",
    "        # pooling\n",
    "        gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)\n",
    "        # gru_out = gru_out[:,-1]\n",
    "\n",
    "        return gru_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        code = self.encode(x)\n",
    "        y = self.hidden2label(code)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx+bs]\n",
    "    x, labels = [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x.append(item['code'])\n",
    "        labels.append([item['label']])\n",
    "    return x, torch.FloatTensor(labels)\n",
    "\n",
    "\n",
    "def get_context_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx + bs]\n",
    "    # x1, x2, x3, y1, y2, y3, labels = [], [], [], [], [], [], []\n",
    "    x1, x2, y1, y2, labels = [], [], [], [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x1.append(item['code_x'])\n",
    "        # x2.append(item['calling_x'])\n",
    "        # x3.append(item['called_x'])\n",
    "        x2.append(item['code_versions_x'])\n",
    "\n",
    "        y1.append(item['code_y'])\n",
    "        # y2.append(item['calling_y'])\n",
    "        # y3.append(item['called_y'])\n",
    "        y2.append(item['code_versions_y'])\n",
    "\n",
    "        labels.append([item['label']])\n",
    "    # return x1, x2, x3, y1, y2, y3, torch.FloatTensor(labels)\n",
    "    return x1, x2, y1, y2, torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for classification\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/classification/train_df.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(RANDOM_SEED)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTrain for classification\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m train_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_pickle(DATA_DIR \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/train_df.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39mRANDOM_SEED)\n\u001b[1;32m      9\u001b[0m dev_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(DATA_DIR \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/dev_df.pkl\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39mRANDOM_SEED)\n\u001b[1;32m     10\u001b[0m test_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_pickle(DATA_DIR \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/test_df.pkl\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, random_state\u001b[39m=\u001b[39mRANDOM_SEED)\n",
      "File \u001b[0;32m~/anaconda3/envs/research/lib/python3.9/site-packages/pandas/io/pickle.py:179\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39m4    4    9\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m excs_to_catch \u001b[39m=\u001b[39m (\u001b[39mAttributeError\u001b[39;00m, \u001b[39mImportError\u001b[39;00m, \u001b[39mModuleNotFoundError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m)\n\u001b[0;32m--> 179\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    180\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    181\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    182\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    183\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    184\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    185\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    186\u001b[0m     \u001b[39m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[39m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[39m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[39m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/research/lib/python3.9/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    869\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/classification/train_df.pkl'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    RANDOM_SEED = 42\n",
    "    DATA_DIR = '../data/classification'\n",
    "    MODEL_DIR = '../models'\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "    print(\"Train for classification\")\n",
    "    train_data = pd.read_pickle(DATA_DIR + '/train_df.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "    dev_data = pd.read_pickle(DATA_DIR + '/dev_df.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "    test_data = pd.read_pickle(DATA_DIR + '/test_df.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "    word2vec = Word2Vec.load(DATA_DIR + '/node_w2v_128').wv\n",
    "    MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "    EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "    embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "    embeddings[:word2vec.vectors.shape[0]] = word2vec.vectors\n",
    "\n",
    "    HIDDEN_DIM = 100\n",
    "    ENCODE_DIM = 128\n",
    "    LABELS = 11\n",
    "    EPOCHS = 20\n",
    "    BATCH_SIZE = 64\n",
    "    USE_GPU = True\n",
    "\n",
    "    model = BatchProgramClassifier(EMBEDDING_DIM, HIDDEN_DIM, MAX_TOKENS+1, ENCODE_DIM, LABELS, BATCH_SIZE,\n",
    "                                   USE_GPU, embeddings)\n",
    "\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    parameters = model.parameters()\n",
    "    optimizer = torch.optim.Adamax(parameters, lr=p_learning_rate)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # print(train_data)\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    print('Start training...')\n",
    "\n",
    "    # training procedure\n",
    "    # state_dict = torch.load(MODEL_DIR + '/class_pure_code.pkl')\n",
    "    # model.load_state_dict(state_dict)\n",
    "    best_loss = 10\n",
    "    best_model = None\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        # training epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        i = 0\n",
    "        while i < len(train_data):\n",
    "            batch = get_batch(train_data, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            train_code, train_labels = batch\n",
    "            if USE_GPU:\n",
    "                # train1_inputs, train2_inputs, train_labels = train1_inputs, train2_inputs, train_labels.cuda()\n",
    "                train_code, train_labels = train_code, train_labels.cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            model.batch_size = len(train_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(train_code)\n",
    "\n",
    "            train_labels = train_labels.squeeze()\n",
    "            loss = loss_function(output, train_labels.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            log_prediction = torch.softmax(output, dim=1)\n",
    "            predicted = torch.argmax(log_prediction, dim=1)\n",
    "            for idx in range(len(predicted)):\n",
    "                if predicted[idx] == train_labels[idx]:\n",
    "                    total_acc += 1\n",
    "            total += len(train_labels)\n",
    "            total_loss += loss.item() * len(train_labels)\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = total_acc / total\n",
    "\n",
    "        # dev epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        i = 0\n",
    "        while i < len(dev_data):\n",
    "            batch = get_batch(dev_data, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            dev_code, dev_labels = batch\n",
    "            # val_inputs, val_labels = batch\n",
    "            if USE_GPU:\n",
    "                # val_inputs, val_labels = val_inputs, val_labels.cuda()\n",
    "                dev_code, dev_labels = dev_code, dev_labels.cuda()\n",
    "\n",
    "            model.batch_size = len(dev_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            output = model(dev_code)\n",
    "\n",
    "            dev_labels = dev_labels.squeeze()\n",
    "            loss = loss_function(output, dev_labels.long())\n",
    "\n",
    "            log_prediction = torch.softmax(output, dim=1)\n",
    "            predicted = torch.argmax(log_prediction, dim=1)\n",
    "            for idx in range(len(predicted)):\n",
    "                if predicted[idx] == dev_labels[idx]:\n",
    "                    total_acc += 1\n",
    "            total += len(dev_labels)\n",
    "            total_loss += loss.item() * len(dev_labels)\n",
    "        epoch_loss = total_loss / total\n",
    "        epoch_acc = total_acc / total\n",
    "        end_time = time.time()\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model = model\n",
    "        print('[Epoch: %3d/%3d] Train Loss: %.4f, Validation Loss: %.4f, '\n",
    "              'Train Acc: %.3f, Validation Acc: %.3f, Time Cost: %.3f s'\n",
    "              % (epoch + 1, EPOCHS, train_loss, epoch_loss, train_acc,\n",
    "                 epoch_acc, end_time - start_time))\n",
    "\n",
    "    model = best_model\n",
    "    torch.save(model.state_dict(), MODEL_DIR + '/class_pure_code.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    test\n",
    "    \"\"\"\n",
    "    # model = BatchProgramClassifier(EMBEDDING_DIM, HIDDEN_DIM, MAX_TOKENS + 1, ENCODE_DIM, LABELS, BATCH_SIZE,\n",
    "    #                                USE_GPU, embeddings)\n",
    "    # model.load_state_dict(torch.load(MODEL_DIR + '/class_pure_code.pth'))\n",
    "\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    # testing procedure\n",
    "    predicts = []\n",
    "    trues = []\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    while i < len(test_data):\n",
    "        batch = get_batch(test_data, i, BATCH_SIZE)\n",
    "        i += BATCH_SIZE\n",
    "        # test1_inputs, test2_inputs, test_labels = batch\n",
    "        test_code, test_labels = batch\n",
    "        if USE_GPU:\n",
    "            test_labels = test_labels.cuda()\n",
    "\n",
    "        model.batch_size = len(test_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model(test_code)\n",
    "\n",
    "        test_labels = test_labels.squeeze()\n",
    "        log_prediction = torch.softmax(output, dim=1)\n",
    "        predicted = torch.argmax(log_prediction, dim=1)\n",
    "        for idx in range(len(predicted)):\n",
    "            if predicted[idx] == test_labels[idx]:\n",
    "                total_acc += 1\n",
    "        total += len(test_labels)\n",
    "        #         predicted = (output.data > 0.5).cpu().numpy()\n",
    "        predicts.extend(predicted.cpu().detach().numpy())\n",
    "        trues.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    acc = total_acc / total\n",
    "    p, r, f, _ = precision_recall_fscore_support(trues, predicts, average='macro')\n",
    "\n",
    "    print(\"Total testing results(acc,P,R,F1):%.3f, %.3f, %.3f, %.3f\" % (acc, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
