{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, encode_dim, batch_size, use_gpu, pretrained_weight=None):\n",
    "        super(BatchTreeEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.W_c = nn.Linear(embedding_dim, encode_dim)\n",
    "        self.activation = F.relu\n",
    "        self.stop = -1\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "        self.node_list = []\n",
    "        self.th = torch.cuda if use_gpu else torch\n",
    "        self.batch_node = None\n",
    "        self.max_index = vocab_size\n",
    "        # pretrained  embedding\n",
    "        if pretrained_weight is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "            # self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def create_tensor(self, tensor):\n",
    "        if self.use_gpu:\n",
    "            return tensor.cuda()\n",
    "        return tensor\n",
    "\n",
    "    def traverse_mul(self, node, batch_index):\n",
    "        size = len(node)\n",
    "        if not size:\n",
    "            return None\n",
    "        batch_current = self.create_tensor(Variable(torch.zeros(size, self.embedding_dim)))\n",
    "\n",
    "        index, children_index = [], []\n",
    "        current_node, children = [], []\n",
    "        for i in range(size):\n",
    "            # if node[i][0] is not -1:\n",
    "                index.append(i)\n",
    "                current_node.append(node[i][0])\n",
    "                temp = node[i][1:]\n",
    "                c_num = len(temp)\n",
    "                for j in range(c_num):\n",
    "                    if temp[j][0] != -1:\n",
    "                        if len(children_index) <= j:\n",
    "                            children_index.append([i])\n",
    "                            children.append([temp[j]])\n",
    "                        else:\n",
    "                            children_index[j].append(i)\n",
    "                            children[j].append(temp[j])\n",
    "            # else:\n",
    "            #     batch_index[i] = -1\n",
    "\n",
    "        batch_current = self.W_c(batch_current.index_copy(0, Variable(self.th.LongTensor(index)),\n",
    "                                                          self.embedding(Variable(self.th.LongTensor(current_node)))))\n",
    "\n",
    "        for c in range(len(children)):\n",
    "            zeros = self.create_tensor(Variable(torch.zeros(size, self.encode_dim)))\n",
    "            batch_children_index = [batch_index[i] for i in children_index[c]]\n",
    "            tree = self.traverse_mul(children[c], batch_children_index)\n",
    "            if tree is not None:\n",
    "                batch_current += zeros.index_copy(0, Variable(self.th.LongTensor(children_index[c])), tree)\n",
    "        # batch_index = [i for i in batch_index if i is not -1]\n",
    "        b_in = Variable(self.th.LongTensor(batch_index))\n",
    "        self.node_list.append(self.batch_node.index_copy(0, b_in, batch_current))\n",
    "        return batch_current\n",
    "\n",
    "    def forward(self, x, bs):\n",
    "        self.batch_size = bs\n",
    "        self.batch_node = self.create_tensor(Variable(torch.zeros(self.batch_size, self.encode_dim)))\n",
    "        self.node_list = []\n",
    "        self.traverse_mul(x, list(range(self.batch_size)))\n",
    "        self.node_list = torch.stack(self.node_list)\n",
    "        return torch.max(self.node_list, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProgramClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, encode_dim, label_size, batch_size,\n",
    "                 use_gpu=True, pretrained_weight=None):\n",
    "        super(BatchProgramClassifier, self).__init__()\n",
    "        self.additionl = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.additionr = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.stop = [vocab_size-1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = 1\n",
    "        self.gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.label_size = label_size\n",
    "        self.encoder = BatchTreeEncoder(self.vocab_size, self.embedding_dim, self.encode_dim,\n",
    "                                        self.batch_size, self.gpu, pretrained_weight)\n",
    "        self.root2label = nn.Linear(self.encode_dim, self.label_size)\n",
    "        # gru\n",
    "        self.bigru = nn.GRU(self.encode_dim, self.hidden_dim, num_layers=self.num_layers, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        # linear\n",
    "        self.hidden2label = nn.Linear(self.hidden_dim * 2, self.label_size)\n",
    "        # hidden\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.gpu is True:\n",
    "            if isinstance(self.bigru, nn.LSTM):\n",
    "                h0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                c0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                return h0, c0\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def get_zeros(self, num):\n",
    "        zeros = Variable(torch.zeros(num, self.encode_dim))\n",
    "        if self.gpu:\n",
    "            return zeros.cuda()\n",
    "        return zeros\n",
    "\n",
    "    def encode(self, x):\n",
    "        lens = [len(item) for item in x]\n",
    "        max_len = max(lens)\n",
    "\n",
    "        encodes = []\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(lens[i]):\n",
    "                encodes.append(x[i][j])\n",
    "\n",
    "        encodes = self.encoder(encodes, sum(lens))\n",
    "        seq, start, end = [], 0, 0\n",
    "        for i in range(self.batch_size):\n",
    "            end += lens[i]\n",
    "            if max_len-lens[i]:\n",
    "                seq.append(self.get_zeros(max_len-lens[i]))\n",
    "            seq.append(encodes[start:end])\n",
    "            start = end\n",
    "        encodes = torch.cat(seq)\n",
    "        encodes = encodes.view(self.batch_size, max_len, -1)\n",
    "        # return encodes\n",
    "\n",
    "        gru_out, hidden = self.bigru(encodes, self.hidden)\n",
    "        gru_out = torch.transpose(gru_out, 1, 2)\n",
    "        # pooling\n",
    "        gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)\n",
    "        # gru_out = gru_out[:,-1]\n",
    "\n",
    "        return gru_out\n",
    "\n",
    "    # def forward(self, x1, x2, x3, y1, y2, y3):\n",
    "    def forward(self, x1, x2, y1, y2):\n",
    "        l_code, r_code = self.encode(x1), self.encode(y1)\n",
    "        # l_calling, r_calling = self.encode(x2), self.encode(y2)\n",
    "        # l_called, r_called = self.encode(x3), self.encode(y3)\n",
    "        l_code_versions, r_code_versions = self.encode(x2), self.encode(y2)\n",
    "\n",
    "        # l_diff = torch.add(l_calling, -l_called)\n",
    "        # r_diff = torch.add(r_calling, -r_called)\n",
    "\n",
    "        # r = self.context_weight\n",
    "        # lvec = torch.add((1 - r) * l_code, r * l_diff)\n",
    "        # rvec = torch.add((1 - r) * r_code, r * r_diff)\n",
    "\n",
    "        lvec = torch.cat([l_code, l_code_versions], 0)\n",
    "        rvec = torch.cat([r_code, r_code_versions], 0)\n",
    "\n",
    "        # lvec = lvec.view(3, self.batch_size, self.hidden_dim * 2)\n",
    "        # rvec = rvec.view(3, self.batch_size, self.hidden_dim * 2)\n",
    "        lvec = lvec.view(2, self.batch_size, self.hidden_dim * 2)\n",
    "        rvec = rvec.view(2, self.batch_size, self.hidden_dim * 2)\n",
    "        \n",
    "        lvec = torch.max(lvec, dim=0).values\n",
    "        rvec = torch.max(rvec, dim=0).values\n",
    "        # lvec = self.additionl(lvec)\n",
    "        # rvec = self.additionr(rvec)\n",
    "\n",
    "        abs_dist = torch.abs(torch.add(lvec, -rvec))\n",
    "        # contexted_dis = torch.cat([abs_dist, l_calling, r_calling, l_called, r_called], 1)\n",
    "\n",
    "        y = self.hidden2label(abs_dist)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx+bs]\n",
    "    # x1, x2, x3, y1, y2, y3, labels = [], [], [], [], [], [], []\n",
    "    x1, x2, y1, y2, labels = [], [], [], [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x1.append(item['code_x'])\n",
    "        # x2.append(item['calling_x'])\n",
    "        # x3.append(item['called_x'])\n",
    "        x2.append(item['code_versions_x'])\n",
    "\n",
    "        y1.append(item['code_y'])\n",
    "        # y2.append(item['calling_y'])\n",
    "        # y3.append(item['called_y'])\n",
    "        y2.append(item['code_versions_y'])\n",
    "\n",
    "        labels.append([item['label']])\n",
    "    # return x1, x2, x3, y1, y2, y3, torch.FloatTensor(labels)\n",
    "    return x1, x2, y1, y2, torch.FloatTensor(labels)\n",
    "\n",
    "\n",
    "def get_context_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx + bs]\n",
    "    # x1, x2, x3, y1, y2, y3, labels = [], [], [], [], [], [], []\n",
    "    x1, x2, y1, y2, labels = [], [], [], [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x1.append(item['code_x'])\n",
    "        # x2.append(item['calling_x'])\n",
    "        # x3.append(item['called_x'])\n",
    "        x2.append(item['code_version_x'])\n",
    "        \n",
    "        y1.append(item['code_y'])\n",
    "        # y2.append(item['calling_y'])\n",
    "        # y3.append(item['called_y'])\n",
    "        y2.append(item['code_versions_y'])\n",
    "        \n",
    "        labels.append([item['label']])\n",
    "    # return x1, x2, x3, y1, y2, y3, torch.FloatTensor(labels)\n",
    "    return x1, x2, y1, y2, torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for clone detection\n",
      "Start training...\n",
      "[Epoch:   1/ 20] Train Loss: 0.5643, Validation Loss: 0.4666, Train Acc: 0.697, Validation Acc: 0.765, Time Cost: 16.939 s\n",
      "[Epoch:   2/ 20] Train Loss: 0.5012, Validation Loss: 0.4268, Train Acc: 0.707, Validation Acc: 0.765, Time Cost: 17.431 s\n",
      "[Epoch:   3/ 20] Train Loss: 0.4518, Validation Loss: 0.3955, Train Acc: 0.716, Validation Acc: 0.776, Time Cost: 16.741 s\n",
      "[Epoch:   4/ 20] Train Loss: 0.4130, Validation Loss: 0.3774, Train Acc: 0.764, Validation Acc: 0.812, Time Cost: 16.131 s\n",
      "[Epoch:   5/ 20] Train Loss: 0.3725, Validation Loss: 0.3543, Train Acc: 0.802, Validation Acc: 0.835, Time Cost: 16.544 s\n",
      "[Epoch:   6/ 20] Train Loss: 0.3377, Validation Loss: 0.3506, Train Acc: 0.851, Validation Acc: 0.859, Time Cost: 17.643 s\n",
      "[Epoch:   7/ 20] Train Loss: 0.3005, Validation Loss: 0.3404, Train Acc: 0.890, Validation Acc: 0.859, Time Cost: 16.774 s\n",
      "[Epoch:   8/ 20] Train Loss: 0.2682, Validation Loss: 0.3391, Train Acc: 0.914, Validation Acc: 0.835, Time Cost: 16.680 s\n",
      "[Epoch:   9/ 20] Train Loss: 0.2347, Validation Loss: 0.3327, Train Acc: 0.931, Validation Acc: 0.847, Time Cost: 17.554 s\n",
      "[Epoch:  10/ 20] Train Loss: 0.2051, Validation Loss: 0.3316, Train Acc: 0.943, Validation Acc: 0.847, Time Cost: 17.126 s\n",
      "[Epoch:  11/ 20] Train Loss: 0.1746, Validation Loss: 0.3383, Train Acc: 0.956, Validation Acc: 0.835, Time Cost: 17.428 s\n",
      "[Epoch:  12/ 20] Train Loss: 0.1506, Validation Loss: 0.3430, Train Acc: 0.969, Validation Acc: 0.847, Time Cost: 16.965 s\n",
      "[Epoch:  13/ 20] Train Loss: 0.1280, Validation Loss: 0.3486, Train Acc: 0.980, Validation Acc: 0.847, Time Cost: 16.123 s\n",
      "[Epoch:  14/ 20] Train Loss: 0.1098, Validation Loss: 0.3673, Train Acc: 0.984, Validation Acc: 0.859, Time Cost: 16.631 s\n",
      "[Epoch:  15/ 20] Train Loss: 0.0939, Validation Loss: 0.3822, Train Acc: 0.991, Validation Acc: 0.847, Time Cost: 16.935 s\n",
      "[Epoch:  16/ 20] Train Loss: 0.0841, Validation Loss: 0.3838, Train Acc: 0.997, Validation Acc: 0.859, Time Cost: 17.252 s\n",
      "[Epoch:  17/ 20] Train Loss: 0.0849, Validation Loss: 0.3565, Train Acc: 0.997, Validation Acc: 0.847, Time Cost: 16.615 s\n",
      "[Epoch:  18/ 20] Train Loss: 0.0851, Validation Loss: 0.3384, Train Acc: 0.997, Validation Acc: 0.847, Time Cost: 16.730 s\n",
      "[Epoch:  19/ 20] Train Loss: 0.0768, Validation Loss: 0.3687, Train Acc: 0.993, Validation Acc: 0.871, Time Cost: 17.487 s\n",
      "[Epoch:  20/ 20] Train Loss: 0.0553, Validation Loss: 0.3705, Train Acc: 1.000, Validation Acc: 0.859, Time Cost: 16.755 s\n",
      "Total testing results(acc,P,R,F1):0.826, 0.905, 0.594, 0.717\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    RANDOM_SEED = 42\n",
    "    DATA_DIR = '../data/clone_detection'\n",
    "    MODEL_DIR = '../models'\n",
    "\n",
    "    word2vec = Word2Vec.load(DATA_DIR + '/node_w2v_128').wv\n",
    "    MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "    EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "    embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "    embeddings[:word2vec.vectors.shape[0]] = word2vec.vectors\n",
    "\n",
    "    HIDDEN_DIM = 100\n",
    "    ENCODE_DIM = 128\n",
    "    LABELS = 1\n",
    "    EPOCHS = 20\n",
    "    BATCH_SIZE = 64\n",
    "    USE_GPU = True\n",
    "\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "    print(\"Train for clone detection\")\n",
    "    train_data = pd.read_pickle(DATA_DIR + '/train_blocks.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "    dev_data = pd.read_pickle(DATA_DIR + '/dev_blocks.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "    test_data = pd.read_pickle(DATA_DIR + '/test_blocks.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "    train_data.loc[train_data['label'] > 0, 'label'] = 1\n",
    "    dev_data.loc[dev_data['label'] > 0, 'label'] = 1\n",
    "    test_data.loc[test_data['label'] > 0, 'label'] = 1\n",
    "    model = BatchProgramClassifier(EMBEDDING_DIM, HIDDEN_DIM, MAX_TOKENS+1, ENCODE_DIM, LABELS, BATCH_SIZE,\n",
    "                                   USE_GPU, embeddings)\n",
    "\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    parameters = model.parameters()\n",
    "    optimizer = torch.optim.Adamax(parameters, lr=p_learning_rate)\n",
    "    loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # print(train_data)\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    print('Start training...')\n",
    "\n",
    "    # training procedure\n",
    "    # state_dict = torch.load(MODEL_DIR + '/clone_max_pool.pth')\n",
    "    # model.load_state_dict(state_dict)\n",
    "    best_loss = 10\n",
    "    best_model = None\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        # training epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        i = 0\n",
    "        while i < len(train_data):\n",
    "            batch = get_batch(train_data, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            # train_code_x, train_calling_x, train_called_x, train_code_y, train_calling_y, train_called_y, train_labels = batch\n",
    "            train_code_x, train_code_versions_x, train_code_y, train_code_versions_y, train_labels = batch\n",
    "            if USE_GPU:\n",
    "                # train1_inputs, train2_inputs, train_labels = train1_inputs, train2_inputs, train_labels.cuda()\n",
    "                train_labels = train_labels.cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            model.batch_size = len(train_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            # output = model(train_code_x, train_calling_x, train_called_x, train_code_y, train_calling_y, train_called_y)\n",
    "            output = model(train_code_x, train_code_versions_x, train_code_y, train_code_versions_y)\n",
    "\n",
    "            # train_labels = train_labels.squeeze()\n",
    "            loss = loss_function(output, train_labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # output = output.squeeze()\n",
    "            output = torch.sigmoid(output)\n",
    "            predicted = torch.round(output)\n",
    "            for idx in range(len(predicted)):\n",
    "                if predicted[idx] == train_labels[idx]:\n",
    "                    total_acc += 1\n",
    "            total += len(train_labels)\n",
    "            total_loss += loss.item() * len(train_labels)\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = total_acc / total\n",
    "\n",
    "        # dev epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        i = 0\n",
    "        while i < len(dev_data):\n",
    "            batch = get_batch(dev_data, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            # dev_code_x, dev_calling_x, dev_called_x, dev_code_y, dev_calling_y, dev_called_y, dev_labels = batch\n",
    "            dev_code_x, dev_code_version_x, dev_code_y, dev_code_version_y, dev_labels = batch\n",
    "            # val_inputs, val_labels = batch\n",
    "            if USE_GPU:\n",
    "                # val_inputs, val_labels = val_inputs, val_labels.cuda()\n",
    "                dev_labels = dev_labels.cuda()\n",
    "\n",
    "            model.batch_size = len(dev_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            # output = model(dev_code_x, dev_calling_x, dev_called_x, dev_code_y, dev_calling_y, dev_called_y)\n",
    "            output = model(dev_code_x, dev_code_version_x, dev_code_y, dev_code_version_y)\n",
    "\n",
    "            # dev_labels = dev_labels.squeeze()\n",
    "            loss = loss_function(output, dev_labels.float())\n",
    "\n",
    "            # output = output.squeeze()\n",
    "            output = torch.sigmoid(output)\n",
    "            predicted = torch.round(output)\n",
    "            for idx in range(len(predicted)):\n",
    "                if predicted[idx] == dev_labels[idx]:\n",
    "                    total_acc += 1\n",
    "            total += len(dev_labels)\n",
    "            total_loss += loss.item() * len(dev_labels)\n",
    "        epoch_loss = total_loss / total\n",
    "        epoch_acc = total_acc / total\n",
    "        end_time = time.time()\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model = model\n",
    "        print('[Epoch: %3d/%3d] Train Loss: %.4f, Validation Loss: %.4f, '\n",
    "              'Train Acc: %.3f, Validation Acc: %.3f, Time Cost: %.3f s'\n",
    "              % (epoch + 1, EPOCHS, train_loss, epoch_loss, train_acc,\n",
    "                 epoch_acc, end_time - start_time))\n",
    "\n",
    "    model = best_model\n",
    "    torch.save(model.state_dict(), MODEL_DIR + '/clone_max_pool.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    test\n",
    "    \"\"\"\n",
    "    # model = BatchProgramClassifier(EMBEDDING_DIM, HIDDEN_DIM, MAX_TOKENS + 1, ENCODE_DIM, LABELS, BATCH_SIZE,\n",
    "    #                                USE_GPU, embeddings)\n",
    "    # model.load_state_dict(torch.load(MODEL_DIR + '/clone_max_pool.pth'))\n",
    "\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    # testing procedure\n",
    "    predicts = []\n",
    "    trues = []\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    while i < len(test_data):\n",
    "        batch = get_batch(test_data, i, BATCH_SIZE)\n",
    "        i += BATCH_SIZE\n",
    "        # test1_inputs, test2_inputs, test_labels = batch\n",
    "        # test_code_x, test_calling_x, test_called_x, test_code_y, test_calling_y, test_called_y, test_labels = batch\n",
    "        test_code_x, test_code_version_x, test_code_y, test_code_version_y, test_labels = batch\n",
    "        if USE_GPU:\n",
    "            test_labels = test_labels.cuda()\n",
    "\n",
    "        model.batch_size = len(test_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        # output = model(test_code_x, test_calling_x, test_called_x, test_code_y, test_calling_y, test_called_y)\n",
    "        output = model(test_code_x, test_code_version_x, test_code_y, test_code_version_y)\n",
    "\n",
    "        output = torch.sigmoid(output)\n",
    "        predicted = torch.round(output)\n",
    "        for idx in range(len(predicted)):\n",
    "            if predicted[idx] == test_labels[idx]:\n",
    "                total_acc += 1\n",
    "        total += len(test_labels)\n",
    "        #         predicted = (output.data > 0.5).cpu().numpy()\n",
    "        predicts.extend(predicted.cpu().detach().numpy())\n",
    "        trues.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    acc = total_acc / total\n",
    "    p, r, f, _ = precision_recall_fscore_support(trues, predicts, average='binary')\n",
    "\n",
    "    print(\"Total testing results(acc,P,R,F1):%.3f, %.3f, %.3f, %.3f\" % (acc, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for clone detection\n",
      "Start training...\n",
      "[Epoch:   1/ 20] Train Loss: 0.5643, Validation Loss: 0.4666, Train Acc: 0.697, Validation Acc: 0.765, Time Cost: 16.573 s\n",
      "[Epoch:   2/ 20] Train Loss: 0.5012, Validation Loss: 0.4268, Train Acc: 0.707, Validation Acc: 0.765, Time Cost: 16.380 s\n",
      "[Epoch:   3/ 20] Train Loss: 0.4518, Validation Loss: 0.3955, Train Acc: 0.716, Validation Acc: 0.776, Time Cost: 17.803 s\n",
      "[Epoch:   4/ 20] Train Loss: 0.4130, Validation Loss: 0.3774, Train Acc: 0.764, Validation Acc: 0.812, Time Cost: 17.702 s\n",
      "[Epoch:   5/ 20] Train Loss: 0.3725, Validation Loss: 0.3543, Train Acc: 0.802, Validation Acc: 0.835, Time Cost: 17.123 s\n",
      "[Epoch:   6/ 20] Train Loss: 0.3377, Validation Loss: 0.3506, Train Acc: 0.851, Validation Acc: 0.859, Time Cost: 17.504 s\n",
      "[Epoch:   7/ 20] Train Loss: 0.3005, Validation Loss: 0.3404, Train Acc: 0.890, Validation Acc: 0.859, Time Cost: 17.834 s\n",
      "[Epoch:   8/ 20] Train Loss: 0.2682, Validation Loss: 0.3391, Train Acc: 0.914, Validation Acc: 0.835, Time Cost: 17.043 s\n",
      "[Epoch:   9/ 20] Train Loss: 0.2347, Validation Loss: 0.3327, Train Acc: 0.931, Validation Acc: 0.847, Time Cost: 16.351 s\n",
      "[Epoch:  10/ 20] Train Loss: 0.2051, Validation Loss: 0.3316, Train Acc: 0.943, Validation Acc: 0.847, Time Cost: 17.160 s\n",
      "[Epoch:  11/ 20] Train Loss: 0.1746, Validation Loss: 0.3383, Train Acc: 0.956, Validation Acc: 0.835, Time Cost: 16.697 s\n",
      "[Epoch:  12/ 20] Train Loss: 0.1506, Validation Loss: 0.3430, Train Acc: 0.969, Validation Acc: 0.847, Time Cost: 16.903 s\n",
      "[Epoch:  13/ 20] Train Loss: 0.1280, Validation Loss: 0.3486, Train Acc: 0.980, Validation Acc: 0.847, Time Cost: 16.750 s\n",
      "[Epoch:  14/ 20] Train Loss: 0.1098, Validation Loss: 0.3673, Train Acc: 0.984, Validation Acc: 0.859, Time Cost: 16.914 s\n",
      "[Epoch:  15/ 20] Train Loss: 0.0939, Validation Loss: 0.3822, Train Acc: 0.991, Validation Acc: 0.847, Time Cost: 17.053 s\n",
      "[Epoch:  16/ 20] Train Loss: 0.0841, Validation Loss: 0.3838, Train Acc: 0.997, Validation Acc: 0.859, Time Cost: 17.657 s\n",
      "[Epoch:  17/ 20] Train Loss: 0.0849, Validation Loss: 0.3565, Train Acc: 0.997, Validation Acc: 0.847, Time Cost: 17.623 s\n",
      "[Epoch:  18/ 20] Train Loss: 0.0851, Validation Loss: 0.3384, Train Acc: 0.997, Validation Acc: 0.847, Time Cost: 17.205 s\n",
      "[Epoch:  19/ 20] Train Loss: 0.0768, Validation Loss: 0.3687, Train Acc: 0.993, Validation Acc: 0.871, Time Cost: 16.524 s\n",
      "[Epoch:  20/ 20] Train Loss: 0.0553, Validation Loss: 0.3705, Train Acc: 1.000, Validation Acc: 0.859, Time Cost: 17.729 s\n",
      "Total testing results(acc,P,R,F1):0.826, 0.905, 0.594, 0.717\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    RANDOM_SEED = 42\n",
    "    DATA_DIR = '../data/clone_detection'\n",
    "    MODEL_DIR = '../models'\n",
    "\n",
    "    word2vec = Word2Vec.load(DATA_DIR + '/node_w2v_128').wv\n",
    "    MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "    EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "    embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "    embeddings[:word2vec.vectors.shape[0]] = word2vec.vectors\n",
    "\n",
    "    HIDDEN_DIM = 100\n",
    "    ENCODE_DIM = 128\n",
    "    LABELS = 1\n",
    "    EPOCHS = 20\n",
    "    BATCH_SIZE = 64\n",
    "    USE_GPU = True\n",
    "\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "    print(\"Train for clone detection\")\n",
    "    train_data = pd.read_pickle(DATA_DIR + '/train_blocks.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "    dev_data = pd.read_pickle(DATA_DIR + '/dev_blocks.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "    test_data = pd.read_pickle(DATA_DIR + '/test_blocks.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "    train_data.loc[train_data['label'] > 0, 'label'] = 1\n",
    "    dev_data.loc[dev_data['label'] > 0, 'label'] = 1\n",
    "    test_data.loc[test_data['label'] > 0, 'label'] = 1\n",
    "    model = BatchProgramClassifier(EMBEDDING_DIM, HIDDEN_DIM, MAX_TOKENS+1, ENCODE_DIM, LABELS, BATCH_SIZE,\n",
    "                                   USE_GPU, embeddings)\n",
    "\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    parameters = model.parameters()\n",
    "    optimizer = torch.optim.Adamax(parameters, lr=p_learning_rate)\n",
    "    loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # print(train_data)\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    print('Start training...')\n",
    "\n",
    "    # training procedure\n",
    "    # state_dict = torch.load(MODEL_DIR + '/clone_max_pool.pth')\n",
    "    # model.load_state_dict(state_dict)\n",
    "    best_loss = 10\n",
    "    best_model = None\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        # training epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        i = 0\n",
    "        while i < len(train_data):\n",
    "            batch = get_batch(train_data, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            # train_code_x, train_calling_x, train_called_x, train_code_y, train_calling_y, train_called_y, train_labels = batch\n",
    "            train_code_x, train_code_versions_x, train_code_y, train_code_versions_y, train_labels = batch\n",
    "            if USE_GPU:\n",
    "                # train1_inputs, train2_inputs, train_labels = train1_inputs, train2_inputs, train_labels.cuda()\n",
    "                train_labels = train_labels.cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            model.batch_size = len(train_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            # output = model(train_code_x, train_calling_x, train_called_x, train_code_y, train_calling_y, train_called_y)\n",
    "            output = model(train_code_x, train_code_versions_x, train_code_y, train_code_versions_y)\n",
    "\n",
    "            # train_labels = train_labels.squeeze()\n",
    "            loss = loss_function(output, train_labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # output = output.squeeze()\n",
    "            output = torch.sigmoid(output)\n",
    "            predicted = torch.round(output)\n",
    "            for idx in range(len(predicted)):\n",
    "                if predicted[idx] == train_labels[idx]:\n",
    "                    total_acc += 1\n",
    "            total += len(train_labels)\n",
    "            total_loss += loss.item() * len(train_labels)\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = total_acc / total\n",
    "\n",
    "        # dev epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        i = 0\n",
    "        while i < len(dev_data):\n",
    "            batch = get_batch(dev_data, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            # dev_code_x, dev_calling_x, dev_called_x, dev_code_y, dev_calling_y, dev_called_y, dev_labels = batch\n",
    "            dev_code_x, dev_code_version_x, dev_code_y, dev_code_version_y, dev_labels = batch\n",
    "            # val_inputs, val_labels = batch\n",
    "            if USE_GPU:\n",
    "                # val_inputs, val_labels = val_inputs, val_labels.cuda()\n",
    "                dev_labels = dev_labels.cuda()\n",
    "\n",
    "            model.batch_size = len(dev_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            # output = model(dev_code_x, dev_calling_x, dev_called_x, dev_code_y, dev_calling_y, dev_called_y)\n",
    "            output = model(dev_code_x, dev_code_version_x, dev_code_y, dev_code_version_y)\n",
    "\n",
    "            # dev_labels = dev_labels.squeeze()\n",
    "            loss = loss_function(output, dev_labels.float())\n",
    "\n",
    "            # output = output.squeeze()\n",
    "            output = torch.sigmoid(output)\n",
    "            predicted = torch.round(output)\n",
    "            for idx in range(len(predicted)):\n",
    "                if predicted[idx] == dev_labels[idx]:\n",
    "                    total_acc += 1\n",
    "            total += len(dev_labels)\n",
    "            total_loss += loss.item() * len(dev_labels)\n",
    "        epoch_loss = total_loss / total\n",
    "        epoch_acc = total_acc / total\n",
    "        end_time = time.time()\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model = model\n",
    "        print('[Epoch: %3d/%3d] Train Loss: %.4f, Validation Loss: %.4f, '\n",
    "              'Train Acc: %.3f, Validation Acc: %.3f, Time Cost: %.3f s'\n",
    "              % (epoch + 1, EPOCHS, train_loss, epoch_loss, train_acc,\n",
    "                 epoch_acc, end_time - start_time))\n",
    "\n",
    "    model = best_model\n",
    "    torch.save(model.state_dict(), MODEL_DIR + '/clone_max_pool.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    test\n",
    "    \"\"\"\n",
    "    # model = BatchProgramClassifier(EMBEDDING_DIM, HIDDEN_DIM, MAX_TOKENS + 1, ENCODE_DIM, LABELS, BATCH_SIZE,\n",
    "    #                                USE_GPU, embeddings)\n",
    "    # model.load_state_dict(torch.load(MODEL_DIR + '/clone_max_pool.pth'))\n",
    "\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    # testing procedure\n",
    "    predicts = []\n",
    "    trues = []\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    while i < len(test_data):\n",
    "        batch = get_batch(test_data, i, BATCH_SIZE)\n",
    "        i += BATCH_SIZE\n",
    "        # test1_inputs, test2_inputs, test_labels = batch\n",
    "        # test_code_x, test_calling_x, test_called_x, test_code_y, test_calling_y, test_called_y, test_labels = batch\n",
    "        test_code_x, test_code_version_x, test_code_y, test_code_version_y, test_labels = batch\n",
    "        if USE_GPU:\n",
    "            test_labels = test_labels.cuda()\n",
    "\n",
    "        model.batch_size = len(test_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        # output = model(test_code_x, test_calling_x, test_called_x, test_code_y, test_calling_y, test_called_y)\n",
    "        output = model(test_code_x, test_code_version_x, test_code_y, test_code_version_y)\n",
    "\n",
    "        output = torch.sigmoid(output)\n",
    "        predicted = torch.round(output)\n",
    "        for idx in range(len(predicted)):\n",
    "            if predicted[idx] == test_labels[idx]:\n",
    "                total_acc += 1\n",
    "        total += len(test_labels)\n",
    "        #         predicted = (output.data > 0.5).cpu().numpy()\n",
    "        predicts.extend(predicted.cpu().detach().numpy())\n",
    "        trues.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    acc = total_acc / total\n",
    "    p, r, f, _ = precision_recall_fscore_support(trues, predicts, average='binary')\n",
    "\n",
    "    print(\"Total testing results(acc,P,R,F1):%.3f, %.3f, %.3f, %.3f\" % (acc, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
