{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import warnings\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchTreeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, encode_dim, batch_size, use_gpu, pretrained_weight=None):\n",
    "        super(BatchTreeEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.W_c = nn.Linear(embedding_dim, encode_dim)\n",
    "        self.activation = F.relu\n",
    "        self.stop = -1\n",
    "        self.batch_size = batch_size\n",
    "        self.use_gpu = use_gpu\n",
    "        self.node_list = []\n",
    "        self.th = torch.cuda if use_gpu else torch\n",
    "        self.batch_node = None\n",
    "        self.max_index = vocab_size\n",
    "        # pretrained  embedding\n",
    "        if pretrained_weight is not None:\n",
    "            self.embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "            # self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def create_tensor(self, tensor):\n",
    "        if self.use_gpu:\n",
    "            return tensor.cuda()\n",
    "        return tensor\n",
    "\n",
    "    def traverse_mul(self, node, batch_index):\n",
    "        size = len(node)\n",
    "        if not size:\n",
    "            return None\n",
    "        batch_current = self.create_tensor(Variable(torch.zeros(size, self.embedding_dim)))\n",
    "\n",
    "        index, children_index = [], []\n",
    "        current_node, children = [], []\n",
    "        for i in range(size):\n",
    "            # if node[i][0] is not -1:\n",
    "                index.append(i)\n",
    "                current_node.append(node[i][0])\n",
    "                temp = node[i][1:]\n",
    "                c_num = len(temp)\n",
    "                for j in range(c_num):\n",
    "                    if temp[j][0] != -1:\n",
    "                        if len(children_index) <= j:\n",
    "                            children_index.append([i])\n",
    "                            children.append([temp[j]])\n",
    "                        else:\n",
    "                            children_index[j].append(i)\n",
    "                            children[j].append(temp[j])\n",
    "            # else:\n",
    "            #     batch_index[i] = -1\n",
    "\n",
    "        batch_current = self.W_c(batch_current.index_copy(0, Variable(self.th.LongTensor(index)),\n",
    "                                                          self.embedding(Variable(self.th.LongTensor(current_node)))))\n",
    "\n",
    "        for c in range(len(children)):\n",
    "            zeros = self.create_tensor(Variable(torch.zeros(size, self.encode_dim)))\n",
    "            batch_children_index = [batch_index[i] for i in children_index[c]]\n",
    "            tree = self.traverse_mul(children[c], batch_children_index)\n",
    "            if tree is not None:\n",
    "                batch_current += zeros.index_copy(0, Variable(self.th.LongTensor(children_index[c])), tree)\n",
    "        # batch_index = [i for i in batch_index if i is not -1]\n",
    "        b_in = Variable(self.th.LongTensor(batch_index))\n",
    "        self.node_list.append(self.batch_node.index_copy(0, b_in, batch_current))\n",
    "        return batch_current\n",
    "\n",
    "    def forward(self, x, bs):\n",
    "        self.batch_size = bs\n",
    "        self.batch_node = self.create_tensor(Variable(torch.zeros(self.batch_size, self.encode_dim)))\n",
    "        self.node_list = []\n",
    "        self.traverse_mul(x, list(range(self.batch_size)))\n",
    "        self.node_list = torch.stack(self.node_list)\n",
    "        return torch.max(self.node_list, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchProgramClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, encode_dim, label_size, batch_size,\n",
    "                 use_gpu=True, pretrained_weight=None):\n",
    "        super(BatchProgramClassifier, self).__init__()\n",
    "        self.additionl = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.additionr = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "        self.stop = [vocab_size-1]\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = 1\n",
    "        self.gpu = use_gpu\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        self.label_size = label_size\n",
    "        self.encoder = BatchTreeEncoder(self.vocab_size, self.embedding_dim, self.encode_dim,\n",
    "                                        self.batch_size, self.gpu, pretrained_weight)\n",
    "        self.root2label = nn.Linear(self.encode_dim, self.label_size)\n",
    "        # gru\n",
    "        self.bigru = nn.GRU(self.encode_dim, self.hidden_dim, num_layers=self.num_layers, bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        # linear\n",
    "        # self.hidden2label = nn.Linear(self.hidden_dim * 2 * 3, self.label_size)        \n",
    "        self.hidden2label = nn.Linear(400, self.label_size)        \n",
    "\n",
    "        # hidden\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        if self.gpu is True:\n",
    "            if isinstance(self.bigru, nn.LSTM):\n",
    "                h0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                c0 = Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim).cuda())\n",
    "                return h0, c0\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim)).cuda()\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.num_layers * 2, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def get_zeros(self, num):\n",
    "        zeros = Variable(torch.zeros(num, self.encode_dim))\n",
    "        if self.gpu:\n",
    "            return zeros.cuda()\n",
    "        return zeros\n",
    "\n",
    "    def encode(self, x):\n",
    "        lens = [len(item) for item in x]\n",
    "        max_len = max(lens)\n",
    "\n",
    "        encodes = []\n",
    "        for i in range(self.batch_size):\n",
    "            for j in range(lens[i]):\n",
    "                encodes.append(x[i][j])\n",
    "\n",
    "        encodes = self.encoder(encodes, sum(lens))\n",
    "        seq, start, end = [], 0, 0\n",
    "        for i in range(self.batch_size):\n",
    "            end += lens[i]\n",
    "            if max_len-lens[i]:\n",
    "                seq.append(self.get_zeros(max_len-lens[i]))\n",
    "            seq.append(encodes[start:end])\n",
    "            start = end\n",
    "        encodes = torch.cat(seq)\n",
    "        encodes = encodes.view(self.batch_size, max_len, -1)\n",
    "        # return encodes\n",
    "\n",
    "        gru_out, hidden = self.bigru(encodes, self.hidden)\n",
    "        gru_out = torch.transpose(gru_out, 1, 2)\n",
    "        # pooling\n",
    "        gru_out = F.max_pool1d(gru_out, gru_out.size(2)).squeeze(2)\n",
    "        # gru_out = gru_out[:,-1]\n",
    "\n",
    "        return gru_out\n",
    "\n",
    "    # def forward(self, x1, x2, x3, y1, y2, y3):\n",
    "    def forward(self, x1, x2, y1, y2):\n",
    "        l_code, r_code = self.encode(x1), self.encode(y1)\n",
    "\n",
    "        # l_calling, r_calling = self.encode(x2), self.encode(y2)\n",
    "        # l_called, r_called = self.encode(x3), self.encode(y3)\n",
    "        l_code_versions, r_code_versions = self.encode(x2), self.encode(y2)\n",
    "\n",
    "        # l_diff = torch.add(l_calling, -l_called)\n",
    "        # r_diff = torch.add(r_calling, -r_called)\n",
    "\n",
    "        # r = self.context_weight\n",
    "        # lvec = torch.add((1 - r) * l_code, r * l_diff)\n",
    "        # rvec = torch.add((1 - r) * r_code, r * r_diff)\n",
    "\n",
    "        lvec = torch.cat([l_code, l_code_versions], 1)\n",
    "        rvec = torch.cat([r_code, r_code_versions], 1)\n",
    "        # lvec = self.additionl(lvec)\n",
    "        # rvec = self.additionr(rvec)\n",
    "\n",
    "        abs_dist = torch.abs(torch.add(lvec, -rvec))\n",
    "        # contexted_dis = torch.cat([abs_dist, l_calling, r_calling, l_called, r_called], 1)\n",
    "        # breakpoint()\n",
    "\n",
    "        y = self.hidden2label(abs_dist)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx+bs]\n",
    "    # x1, x2, x3, y1, y2, y3, labels = [], [], [], [], [], [], []\n",
    "    x1, x2, y1, y2, labels = [], [], [], [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x1.append(item['code_x'])\n",
    "        # x2.append(item['calling_x'])\n",
    "        # x3.append(item['called_x'])\n",
    "        x2.append(item['code_versions_x'])\n",
    "\n",
    "        y1.append(item['code_y'])\n",
    "        # y2.append(item['calling_y'])\n",
    "        # y3.append(item['called_y'])\n",
    "        y2.append(item['code_versions_y'])\n",
    "        labels.append([item['label']])\n",
    "    # return x1, x2, x3, y1, y2, y3, torch.FloatTensor(labels)\n",
    "    return x1, x2, y1, y2, torch.FloatTensor(labels)\n",
    "\n",
    "\n",
    "def get_context_batch(dataset, idx, bs):\n",
    "    tmp = dataset.iloc[idx: idx + bs]\n",
    "    # x1, x2, x3, y1, y2, y3, labels = [], [], [], [], [], [], []\n",
    "    x1, x2, y1, y2, labels = [], [], [], [], []\n",
    "    for _, item in tmp.iterrows():\n",
    "        x1.append(item['code_x'])\n",
    "        # x2.append(item['calling_x'])\n",
    "        # x3.append(item['called_x'])\n",
    "        x2.append(item['code_versions_x'])\n",
    "\n",
    "        y1.append(item['code_y'])\n",
    "        # y2.append(item['calling_y'])\n",
    "        # y3.append(item['called_y'])\n",
    "        y2.append(item['code_versions_y'])\n",
    "\n",
    "        labels.append([item['label']])\n",
    "    # return x1, x2, x3, y1, y2, y3, torch.FloatTensor(labels)\n",
    "    return x1, x2, y1, y2, torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_TOKENS 2972\n",
      "EMBEDDING_DIM 128\n",
      "Train for clone detection\n",
      "Start training...\n",
      "tensor(0.7680, device='cuda:0',\n",
      "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m loss \u001b[39m=\u001b[39m loss_function(output, train_labels\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     75\u001b[0m \u001b[39mprint\u001b[39m(loss)\n\u001b[0;32m---> 76\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     77\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     79\u001b[0m \u001b[39m# output = output.squeeze()\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/research/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/research/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    RANDOM_SEED = 42\n",
    "    DATA_DIR = '../data/clone_detection'\n",
    "    MODEL_DIR = '../models'\n",
    "\n",
    "    word2vec = Word2Vec.load(DATA_DIR + '/node_w2v_128').wv\n",
    "    MAX_TOKENS = word2vec.vectors.shape[0]\n",
    "    print(\"MAX_TOKENS\",MAX_TOKENS)\n",
    "    EMBEDDING_DIM = word2vec.vectors.shape[1]\n",
    "    print(\"EMBEDDING_DIM\", EMBEDDING_DIM)\n",
    "    embeddings = np.zeros((MAX_TOKENS + 1, EMBEDDING_DIM), dtype=\"float32\")\n",
    "    embeddings[:word2vec.vectors.shape[0]] = word2vec.vectors\n",
    "\n",
    "    HIDDEN_DIM = 100\n",
    "    ENCODE_DIM = 64\n",
    "    LABELS = 1\n",
    "    EPOCHS = 20\n",
    "    BATCH_SIZE = 1\n",
    "    USE_GPU = True\n",
    "\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "    print(\"Train for clone detection\")\n",
    "    train_data = pd.read_pickle(DATA_DIR + '/train_blocks.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "    dev_data = pd.read_pickle(DATA_DIR + '/dev_blocks.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "    test_data = pd.read_pickle(DATA_DIR + '/test_blocks.pkl').sample(frac=1, random_state=RANDOM_SEED)\n",
    "\n",
    "    train_data.loc[train_data['label'] > 0, 'label'] = 1\n",
    "    dev_data.loc[dev_data['label'] > 0, 'label'] = 1\n",
    "    test_data.loc[test_data['label'] > 0, 'label'] = 1\n",
    "\n",
    "    model = BatchProgramClassifier(EMBEDDING_DIM, HIDDEN_DIM, MAX_TOKENS+1, ENCODE_DIM, LABELS, BATCH_SIZE,\n",
    "                                   USE_GPU, embeddings)\n",
    "\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    parameters = model.parameters()\n",
    "    optimizer = torch.optim.Adamax(parameters, lr=p_learning_rate)\n",
    "    loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # print(train_data)\n",
    "    precision, recall, f1 = 0, 0, 0\n",
    "    print('Start training...')\n",
    "\n",
    "    # training procedure\n",
    "    # state_dict = torch.load(MODEL_DIR + '/clone_concat.pth')\n",
    "    # model.load_state_dict(state_dict)\n",
    "    best_loss = 10\n",
    "    best_model = None\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        # training epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        i = 0\n",
    "        while i < len(train_data):\n",
    "            batch = get_batch(train_data, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            # train_code_x, train_calling_x, train_called_x, train_code_y, train_calling_y, train_called_y, train_labels = batch\n",
    "            train_code_x, train_code_versions_x, train_code_y, train_code_versions_y, train_labels = batch\n",
    "            if USE_GPU:\n",
    "                # train1_inputs, train2_inputs, train_labels = train1_inputs, train2_inputs, train_labels.cuda()\n",
    "                train_labels = train_labels.cuda()\n",
    "\n",
    "            model.zero_grad()\n",
    "            model.batch_size = len(train_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            # output = model(train_code_x, train_calling_x, train_called_x, train_code_y, train_calling_y, train_called_y)\n",
    "            output = model(train_code_x, train_code_versions_x, train_code_y, train_code_versions_y)\n",
    "\n",
    "            # train_labels = train_labels.squeeze()\n",
    "            loss = loss_function(output, train_labels.float())\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # output = output.squeeze()\n",
    "            output = torch.sigmoid(output)\n",
    "            predicted = torch.round(output)\n",
    "            for idx in range(len(predicted)):\n",
    "                if predicted[idx] == train_labels[idx]:\n",
    "                    total_acc += 1\n",
    "            total += len(train_labels)\n",
    "            total_loss += loss.item() * len(train_labels)\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = total_acc / total\n",
    "\n",
    "        # dev epoch\n",
    "        total_acc = 0.0\n",
    "        total_loss = 0.0\n",
    "        total = 0.0\n",
    "        i = 0\n",
    "        while i < len(dev_data):\n",
    "            batch = get_batch(dev_data, i, BATCH_SIZE)\n",
    "            i += BATCH_SIZE\n",
    "            # dev_code_x, dev_calling_x, dev_called_x, dev_code_y, dev_calling_y, dev_called_y, dev_labels = batch\n",
    "            dev_code_x, dev_code_versions_x, dev_code_y, dev_code_versions_y, dev_labels = batch\n",
    "            # val_inputs, val_labels = batch\n",
    "            if USE_GPU:\n",
    "                # val_inputs, val_labels = val_inputs, val_labels.cuda()\n",
    "                dev_labels = dev_labels.cuda()\n",
    "\n",
    "            model.batch_size = len(dev_labels)\n",
    "            model.hidden = model.init_hidden()\n",
    "            # output = model(dev_code_x, dev_calling_x, dev_called_x, dev_code_y, dev_calling_y, dev_called_y)\n",
    "            output = model(dev_code_x, dev_code_versions_x, dev_code_y, dev_code_versions_y)\n",
    "\n",
    "            # dev_labels = dev_labels.squeeze()\n",
    "            loss = loss_function(output, dev_labels.float())\n",
    "\n",
    "            # output = output.squeeze()\n",
    "            output = torch.sigmoid(output)\n",
    "            predicted = torch.round(output)\n",
    "            for idx in range(len(predicted)):\n",
    "                if predicted[idx] == dev_labels[idx]:\n",
    "                    total_acc += 1\n",
    "            total += len(dev_labels)\n",
    "            total_loss += loss.item() * len(dev_labels)\n",
    "        epoch_loss = total_loss / total\n",
    "        epoch_acc = total_acc / total\n",
    "        end_time = time.time()\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_model = model\n",
    "        print('[Epoch: %3d/%3d] Train Loss: %.4f, Validation Loss: %.4f, '\n",
    "              'Train Acc: %.3f, Validation Acc: %.3f, Time Cost: %.3f s'\n",
    "              % (epoch + 1, EPOCHS, train_loss, epoch_loss, train_acc,\n",
    "                 epoch_acc, end_time - start_time))\n",
    "\n",
    "    model = best_model\n",
    "    torch.save(model.state_dict(), MODEL_DIR + '/clone_concat.pth')\n",
    "\n",
    "    \"\"\"\n",
    "    test\n",
    "    \"\"\"\n",
    "    # model = BatchProgramClassifier(EMBEDDING_DIM, HIDDEN_DIM, MAX_TOKENS + 1, ENCODE_DIM, LABELS, BATCH_SIZE,\n",
    "    #                                USE_GPU, embeddings)\n",
    "    # model.load_state_dict(torch.load(MODEL_DIR + '/clone_concat.pth'))\n",
    "\n",
    "    if USE_GPU:\n",
    "        model.cuda()\n",
    "\n",
    "    # testing procedure\n",
    "    predicts = []\n",
    "    trues = []\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0\n",
    "    total = 0.0\n",
    "    i = 0\n",
    "    while i < len(test_data):\n",
    "        batch = get_batch(test_data, i, BATCH_SIZE)\n",
    "        i += BATCH_SIZE\n",
    "        # test1_inputs, test2_inputs, test_labels = batch\n",
    "        # test_code_x, test_calling_x, test_called_x, test_code_y, test_calling_y, test_called_y, test_labels = batch\n",
    "        test_code_x, test_code_versions_x, test_code_y, test_code_versions_y, test_labels = batch\n",
    "        if USE_GPU:\n",
    "            test_labels = test_labels.cuda()\n",
    "\n",
    "        model.batch_size = len(test_labels)\n",
    "        model.hidden = model.init_hidden()\n",
    "        # output = model(test_code_x, test_calling_x, test_called_x, test_code_y, test_calling_y, test_called_y)\n",
    "        output = model(test_code_x, test_code_versions_x, test_code_y, test_code_versions_y)\n",
    "\n",
    "        output = torch.sigmoid(output)\n",
    "        predicted = torch.round(output)\n",
    "        for idx in range(len(predicted)):\n",
    "            if predicted[idx] == test_labels[idx]:\n",
    "                total_acc += 1\n",
    "        total += len(test_labels)\n",
    "        #         predicted = (output.data > 0.5).cpu().numpy()\n",
    "        predicts.extend(predicted.cpu().detach().numpy())\n",
    "        trues.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    acc = total_acc / total\n",
    "    p, r, f, _ = precision_recall_fscore_support(trues, predicts, average='binary')\n",
    "\n",
    "    print(\"Total testing results(acc,P,R,F1):%.3f, %.3f, %.3f, %.3f\" % (acc, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
